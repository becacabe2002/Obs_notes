![[2_DEingLifecycle-20231208015350577.webp]]

> [!note] Data lifecycle vs Data Engineering Lifecycle
> * DEing lifecycle is just a subset of the whole Data lifecycle (includes Ingestion, Transformation, Serving, Storage)

## Generation
* Origin of the data - source system
	* IoT device
	* Application message queue
	* Transactional database (ref OLTP)

* Need to throughout understanding of how source system work:
	* How it generates data
	* Frequency and velocity of the data
	* Variety of generated data

* Keep an eye on changes in the source system that could break pipelines and analytics.
	Ex: *Migrate the backend to new db technology*

* Two common source systems:
	* **Application db**:
		* Many small service/db pairs with microservices
		![[2_DEingLifecycle-20231208025624967.webp]]
	* **Iot swarms**:
		* A fleet of devices sends data messages to a central collection system.
		![[2_DEingLifecycle-20231208030231608.webp]]

### Evaluating source systems
> [!question] Questions when considering source systems
> - Essentail characteristics of data source? (...)
> - Data presisted in source system? (long term, temporary ...)
> - Data generation rate? (events per second, gb per hour)
> - Level of data consistency?
> - Data contain duplicates?
> - Will some data values arrive late?
> - Schema of the ingested data? (JOIN table ...)
> - If schema changes, how is this dealt with and communicated to downstream stakeholders?
> - How frequent should data be pulled from the source system?
> - If systems are stateful, is data provided as **periodic snapshots** or **update events from change data capture (CDC)**?
> - Data provider for downstream consumption?
> - Will data source performance be impacted from reading?
> - Upstream data dependencies of source systems?

* Each source has unique volume and cadence of data generation.
	* DE needs to understand the limits of the source systems.

* Two popular ***schema*** of source systems:
	* **Schemaless**: 
		* Define schema as data is written (messaging queue, flat file, blob, or document db)
	* **Fixed Schema**:

* Key part of DE's job is takin raw data input in the source system schema and transforming this into valuable output for analytics.
ðŸ‘‰[More on Chap 2.1](obsidian://open?vault=beca_notes&file=Books%2FDE%2FBEG1%20-%20Fundamentals%20of%20Data%20Engineering%2FB%20-%20The%20Data%20Engineering%20Lifecycle%20in%20Depth%2F2.1_DataGenerationInSourceSystems)

## Storage
* Choose right storage solutions is key to success in the rest of data lifecycle.
* A data architecture oftern take advantages of several storage solutions at a time.
* Most of the storage solutions not just function purely as storage.
	* They support complex transformation queries.

* Storage is a stage of data engineering lifecycle, frequently included in other stages (ingestion, transformation, serving)
	-> The way data is stored impacts how it is used.

### Evaluating Storage systems
> [!question] Questions when considering storage system for a data warehouse, data lakehouse, database or object storage
> * Is it compatible with architecture's required write and read speeds? (will it create bottleneck?)
> * How does it handle with **future scaling**? (considering all capacity limits)
> * Will downstream users and processes be able to retrieve data?
> * Is this a pure storage solution (object storage), or does it support complex query patterns (i.e., a cloud data warehouse)?
> * Is the storage system **schema-agnostic** (ob?ect storage)? **flexible schema** (Cassandra)? **enforced schema** (a cloud data warehouse)?
> * How are you tracking master data, golden records data quality, and data lineage for data governance?

### Data access frequency
* Not all data is accessed in the same way.Retrival patterns will based on how data being stored and queried.
* Two types of data based on access frequency:
	* _**Hot data**_: retrieve many times per day, or per second
		* Should be stored in system which supports fast retrieval operation.
	* _**Cold data**_: rarely queried
		* Should be stored in archival system.

> [!warning]
> There is no one-size-fits-all universall storage system.

ðŸ‘‰[More on Chap 2.2 ](obsidian://open?vault=beca_notes&file=Books%2FDE%2FBEG1%20-%20Fundamentals%20of%20Data%20Engineering%2FA%20-%20Foundation%20and%20Building%20Blocks%2F2.2a_Storage)
## Ingestion
* Source systems and ingestion is where most of **bottlenecks** case come from.
	* Source systems become unresponsive or provide poor quality data.
	* Ingestion service might stop working.

### Key considerations for ingestion phase
> [!question] Questions when building ingestion stage
> * Use case for ingesting data? Can i reuse this rather than create multiple versions of the same dataset?
> * Is the data available whenever i need it? (generate system reliability)
> * Data destination after ingestion?
> * Data access frequence?
> * Volum of arrival data?
> * Data format? (can downstream user handle this format?)
> * Is source data in good shape for immediate downstream use?
> * Does it need to be transformed?

### Batch vs Streaming
* Streaming provides data to downstream systems in a continuous, real-time fashion.
	* The latency varies by domain and requirements.

* Batch 
	* Data is ingested:
		* on a predetermined time interval (khoáº£ng thá»i gian Ä‘á»‹nh trÆ°á»›c).
		* fixed size threshold.
	* is a one-way door, the broken down batches inherit the latency of source.
	* is a popular ingestion, especially for analytics and Machine Learning
### Key considerations for _Batch_ vs _Stream ingestion_  
> [!question] Should i go streaming-first or batch-first?

* Can the downstream storage systems handle the rate of real-time streaming?
* Do i need millisecond real-time data ingestion or a micro-batch every minute?
* Are there any benefits of using streaming ingestion in my usecase? What action can I take on that data to be an enhancement upon batch?
* Cost (time, money, ...) of streaming compare to batch?
* Appropriate tools for my use case? (GG Cloud Pub/sub, ... , Kafka, Flink, Spark...)

### Push vs Pull
* **push model** - source system writes data out to a target (db, object store, filesystem...)

Ex: CDC (continuos Change Data Capture)
* *Method1*: Triggers a msg every time a row is changed in source db.
	* Then msg is pushed to a queue.
* *Method2*: record every commit to db to a log.
	* Ingestion system reads the logs, but doesn't directly interact with the db.
	-> No additional load the the source db.

* **pull model** - data retrieved from source system.

Ex: ETL 's extract phase is consist of pull ingestion model.
* ingestion system queries source table to create a *snapshot* on a fixed schedule.

* **streaming ingestion** - data bypasses a backend db and push *directly to an endpoint*.
	* Each recorded reading as an event.
## Transformation
* Data need to be changed from its original form into smth useful for downstream use cases.
* After ingestion, data is mapped into correct types (String -> numeric, date ...)
* Removing bad records
* Transform the data schema and apply **normalization**.
* Aggregation can be used in downstream

> [!question] Considerations in transformation phase
> * Cost and return on investment (ROI) of transformation?
> * Simple and self-isolated as possible?
> * Business rules 
> * Minimizing data movement

* In the future, real-time processing can replace batch processing in certain domains.

* Transformation is often entangled in other phases of the lifecycle.
 Ex: 
 A source system may add event timestamp to a record
 Data is enriched with additional fields and calculations before sending.  
 -> Add value for end consumers of data.

* Data translate business logic into reusable elements

* Data modeling is a clear and current picture of business processes.
	* A raw data needs additional logic of accounting rules.

* Featurization intends to extract an enhance data features useful for training ML models.

## Serving Data
* Data is not consumed or queried is simply useless.

* Data projects must be intentional across the lifecycle.

> [!note]  Some of the most popular usecase
> * Analytics
> * ML
> * reverse ETL

### Analytics
![[1.2_DEingLifecycle-20240220101914058.webp|605]]

* **Business Intelligence, ad hoc**:
* **Operational**:
* **Customer facing**:

### Machine Learning


### Reverse ETL

